---
output:
  html_document
toc-title: 'Results - outcome Euclidean distance (Appendix S5)'

---

---

<!-- loading packages, user-defined functions and datasets -->
```{r echo=FALSE, warning=FALSE, message=FALSE, cache=FALSE, include=FALSE}
lib <- c(
  "ggpattern",
  "beepr", 
  "magrittr", 
  "tidyverse",
  "tokenizers",
  "jsonlite",
  "knitr",
  "ggthemes",
  "lme4",
  "sjPlot",
  "DHARMa"
)

lapply(lib, require, character.only = TRUE)
rm(lib)

load(file="ws_euclidean.RData")

# load libraries and user-defined functions
sapply(
  c("requirements.R", "my_funs.R"),
  source
)
```

<!-- additional functions/objects -->
```{r eval= FALSE, echo = FALSE}
path_results <- str_replace(getwd(), "r_project$", "results/ChiSense-12_for_nlm_extended/1nn")

full_models <- list.dirs(path = path_results, full.names = FALSE) %>%
  str_subset("_random|_downsample", negate = TRUE) %>%
  {.[-1]}

random_models <- list.dirs(path = path_results, full.names = FALSE) %>%
  str_subset("_random")  

test_data <- read_csv("test_utterances.csv") %>%
  mutate(utterance = utterance %>%
           tolower)

bert_output <- list()
for (model_type in full_models) {
  for (target_label in c("band", "bat", "bow", "button", "card",
                         "chicken", "fish", "glasses", "lamb", "letter", "line",
                         "nail", "turkey")) {
    output <- readLines(paste0(path_results, "/", model_type, "/",
                               target_label, ".jsonl")) %>%
      lapply(fromJSON) %>%
      lapply(unlist) %>%
      bind_rows %>%
      unite("utterance", starts_with("tokens"), na.rm = TRUE,  sep = " ") %>% 
      (function(df) {
        df %>%
          select(gold, utterance, matches1, matches3) %>%
          rename(sense = matches1,
                 cosine = matches3) %>%
          rbind(
            df %>%
              select(gold, utterance, matches2, matches4) %>%
              rename(sense = matches2,
                     cosine = matches4)
          )
      }) %>%
      mutate(utterance = utterance %>%
               str_replace_all("<92>", "’")) %>%
      left_join(., test_data, by = "utterance") %>%
      mutate(model = model_type) %>%
      arrange(model_type, study, experiment, condition, utterance) 
    
    bert_output %<>%
      c(list(output))
  }
}

bert_output %<>%
  bind_rows %>%
  mutate(sample = 1) %>%
  mutate(model_type = "full")

bert_output_random <- list()
for (model_type in random_models) {
  for (target_label in c("band", "bat", "bow", "button", "card",
                         "chicken", "fish", "glasses", "lamb", "letter", "line",
                         "nail", "turkey")) {
    output <- readLines(paste0(path_results, "/", model_type, "/",
                               target_label, ".jsonl")) %>%
      lapply(fromJSON) %>%
      lapply(unlist) %>%
      bind_rows %>%
      unite("utterance", starts_with("tokens"), na.rm = TRUE,  sep = " ") %>% 
      (function(df) {
        df %>%
          select(gold, utterance, matches1, matches3) %>%
          rename(sense = matches1,
                 cosine = matches3) %>%
          rbind(
            df %>%
              select(gold, utterance, matches2, matches4) %>%
              rename(sense = matches2,
                     cosine = matches4)
          )
      }) %>%
      mutate(utterance = utterance %>%
               str_replace_all("<92>", "’")) %>%
      left_join(., test_data, by = "utterance") %>%
      mutate(model = model_type) %>%
      arrange(model_type, study, experiment, condition, utterance) 
    
    bert_output_random %<>%
      c(list(output))
  }
}

bert_output_random %<>%
  bind_rows %>%
  separate(model, c("model", "sample"), sep = "_random") %>%
  mutate(model_type = "random")

models_names <- c("distilbert-base-uncased","bert-base-uncased", "bert-large-uncased", "bert-large-uncased-whole-word-masking",
             "distilroberta-base","roberta-base", "roberta-large",
             
             "roberta-med-small-1M-2", "roberta-base-10M-2", "roberta-base-100M-2", "roberta-base-1B-3",
      
            "albert-base-v1", "albert-large-v1", "albert-xlarge-v1", "albert-xxlarge-v1",
            "albert-base-v2", "albert-large-v2", "albert-xlarge-v2",  "albert-xxlarge-v2", 
            
            "deberta-base", "deberta-large", "deberta-xlarge",
            "deberta-v2-xlarge", "deberta-v2-xxlarge", 
            "deberta-v3-small", "deberta-v3-base", "deberta-v3-large",
            
            "babyberta-ao-childes", "babyberta-ao-newsela", "babyberta-wikipedia-1",
            "babyberta-ao-childes-ao-newsela-wikipedia-1", 
            
            "distilgpt2","openai-gpt", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl",
            
            "transfo-xl-wt103",
            
            "ctrl", 
            
            "t5-small", "t5-base", "t5-large",
            
            "xlnet-base-cased", "xlnet-large-cased",
            
            "elmo"
            )

family_codes <- tibble(
  models = models_names) %>%
  mutate(size = c(66, 110, 340, 340, # bert
                  82, 125, 355, # roberta
                  45, 125, 125, 125, # minibertas
                  11, 17, 58, 223, # albert v1
                  11, 17, 58, 223, # albert v2
                  140, 400, 750, # deberta 
                  900, 1500, # deberta v2
                  141, 184, 434, # deberta v3
                  8,8,8,8, # babyberta
                  82, 116, 124, 355, 774, 1558, # gpt
                  284, # transfo-xl
                  1630, # ctrl
                  35, 110, 335, # t5
                  117, 360, # xlnet
                  93 # elmo
                  )) %>%
  mutate(pretraining =
    c(16, 16, 16, 16, # bert
      40, 160, 160, # roberta
      .005, .05, .5, 5, # minibertas
      16, 16, 16, 16, # albert v1
      16, 16, 16, 16, # albert v2
      80, 80, 80, # deberta 
      160, 160, # deberta v2
      160, 160, 160, # deberta v3
      .02,.02,.02,.06, # babyberta
      40, 3, 40, 40, 40, 40, # gpt
      .4, # transfo-xl
      140, # ctrl
      806, 806, 806, # t5
      126, 126, # xlnet
      4.2 # elmo
  )) %>%
  mutate(family = case_when(
    str_detect(models, "^bert-") ~ "bert",
    str_detect(models, "^roberta-") ~ "roberta",
    str_detect(models, "^albert-.*-v1") ~ "albert-v1",
    str_detect(models, "^albert-.*-v2") ~ "albert-v2",
    str_detect(models, "^deberta.*2") ~ "deberta-v2",
    str_detect(models, "^deberta.*3") ~ "deberta-v3",
    str_detect(models, "^deberta[^23]+") ~ "deberta",
    str_detect(models, "^babyberta-") ~ "babyberta",
    str_detect(models, "^gpt|-gpt") ~ "gpt",
    str_detect(models, "^transfo-xl") ~ "transfo-xl",
    str_detect(models, "^ctrl") ~ "ctrl",
    str_detect(models, "^t5-") ~ "t5",
    str_detect(models, "^xlnet-") ~ "xlnet",
    str_detect(models, "^distilbert") ~ "bert",
    str_detect(models, "^distilroberta") ~ "roberta",
    str_detect(models, "^distilgpt2") ~ "gpt",
    str_detect(models, "^elmo") ~ "elmo",
  ))


full_model_levels <- 
  models_names %>%
  rev

random_model_levels <- c("distilbert-base-uncased", "bert-base-uncased", "albert-base-v1", "albert-base-v2", 
                         "roberta-base", "deberta-base", "deberta-v2-xlarge", "deberta-v3-base", "openai-gpt",  "gpt2", "transfo-xl-wt103",
                         "ctrl", "t5-base", "xlnet-base-cased") %>% 
  rev

family_levels <- c("bert", "albert-v1", "albert-v2", "roberta", "deberta", "deberta-v2", "deberta-v3",
                   "babyberta", "gpt", "transfo-xl", "ctrl", "t5", "xlnet", "elmo")

round_anova <- function(df_comparison) {
  df_comparison %>%
    (function(df) {
    df[,-ncol(df)] %>%
      mutate_all(round, 2) %>%
      cbind(
        df[,ncol(df), drop = FALSE] %>%
          mutate_all(round, 3)
      )
  })
} 
```

<!-- save plots to folder -->
```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```

\newline


```{r palette, echo = FALSE}
my_palette <- c(
  "#000000", # bert
  "#4FC601", # albert v1
  "#008941", # albert v2
  "#FFFF00", # roberta
  "#FFDBE5", # deberta
  "#FF4A46", # deberta v2
  "#A30059", # deberta v3
  "#1CE6FF", # babyberta 
  "#0000A6", # gpt
  "#FF34FF", # transfo-xl
  "#6b2a0a", # ctrl
  "#f57402", # t5
  "#848385", # xlnet
  "#7e98f7", # elmo
  "#63FFAC",
  "#B79762",
  "#004D43",
  "#8FB0FF",
  "#997D87",
  "#5A0007",
  "#809693",
  "#FEFFE6",
  "#1B4400",
  "#3B5DFF",
  "#4A3B53",
  "#FF2F80",
  "#61615A",
  "#BA0900",
  "#6B7900",
  "#00C2A0",
  "#FFAA92",
  "#006FA6",
  "#FF90C9",
  "#FF913F",
  "#B903AA",
  "#D16100",
  "#DDEFFF",
  "#000035",
  "#7B4F4B",
  "#A1C299",
  "#300018",
  "#FFB500",
  "#0AA6D8",
  "#013349",
  "#00846F",
  "#372101",
  "#C2FFED",
  "#A079BF",
  "#CC0744",
  "#C0B9B2",
  "#C2FF99",
  "#001E09",
  "#00489C",
  "#6F0062",
  "#0CBD66",
  "#EEC3FF",
  "#456D75",
  "#B77B68",
  "#7A87A1",
  "#788D66",
  "#885578",
  "#FAD09F",
  "#FF8A9A",
  "#D157A0",
  "#BEC459",
  "#456648",
  "#0086ED",
  "#886F4C",
  "#34362D",
  "#B4A8BD",
  "#00A6AA",
  "#452C2C",
  "#636375",
  "#A3C8C9",
  "#938A81",
  "#575329"
)

```

\newline

```{r echo=FALSE, fig.width = 17, fig.height=8, dpi=300}
# save primary sense preference to later use as covariate variable in the stats models. 
primary_preference <- bert_output %>%
  mutate(value = case_when(
    correct == "bow_weapon" ~ 1,
    correct == "bow_knot" ~ 0,
    correct == "line_order" ~ 1,
    correct == "line_geometry" ~ 0,
    TRUE ~ value
  )) %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>%  
  mutate(response = gold == sense) %>%
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(correct = case_when(
    correct == "subordinate" ~ "Subordinate\nplausible",
    correct == "primary" ~ "Dominant\nplausible"
  )) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         correct = factor(correct),
         resp = factor(resp)) %>% 
  group_by(model,  resp) %>%
  summarise(n = length(resp)) %>%
  group_by(model) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>%
  left_join(family_codes, by = c("model" = "models")) %>%  
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>%
  mutate(model = factor(model, levels = full_model_levels)) %>% 
  (function(df) {
    # by model size
    new_levels <- df %>%
      arrange(pretraining) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  rename(preference = perc) %>%
  select(model, preference)
```


\newline

```{r rabagliati_1_example_plot, echo=FALSE, fig.width = 20, fig.height=13, dpi=300}
# approximate maximum differences in primary sense selections in children (Rabagliati et al., 2013; Experiment 1)
current_sentence_primary_plausible <- c(95, 68, 80) %>% mean %>% round(0)
current_sentence_subordinate_plausible <- c(25, 18, 70) %>% mean %>% round(0)

prior_sentence_primary_plausible <- c(89, 68, 80) %>% mean %>% round(0)
prior_sentence_subordinate_plausible <- c(44, 26, 30) %>% mean %>% round(0)

# Rabagliati - no competition
bert_output %>%
  mutate(value = case_when(
    correct == "bow_weapon" ~ 1,
    correct == "bow_knot" ~ 0,
    correct == "line_order" ~ 1,
    correct == "line_geometry" ~ 0,
    TRUE ~ value
  )) %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% 
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(correct = case_when(
    correct == "subordinate" ~ "Subordinate\nplausible",
    correct == "primary" ~ "Dominant\nplausible"
  )) %>%  
  mutate(model = factor(model),
         condition = factor(condition),
         correct = factor(correct),
         resp = factor(resp)) %>% 
  group_by(model, condition, correct, resp) %>% 
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, correct, resp, fill = list(n= 0)) %>% 
  group_by(model, condition, correct) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>%
  rename(`Context-sense association` = correct) %>%
  mutate(condition = str_to_title(condition) %>% 
           str_replace("$", " context")) %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  
  rbind(
    tibble(model = "Children", condition = c(rep("Current context", 2), 
                                             rep("Prior context", 2)), 
           `Context-sense association` = rep(c("Dominant\nplausible", "Subordinate\nplausible"), 2),
           resp = "primary", 
           n = NA,
           tot = NA, 
           perc = c(current_sentence_primary_plausible, current_sentence_subordinate_plausible,
                    prior_sentence_primary_plausible, prior_sentence_subordinate_plausible),
           size = NA,
           pretraining = NA,
           family = "Children")
  ) %>% 
  mutate(model = factor(model, levels = c(full_model_levels, "Children"))) %>%  
  mutate(family = factor(family, levels = c(family_levels, "Children"))) %>%
  filter(model %in% c("Children", "deberta-xlarge", "albert-large-v2")) %>%
  filter(condition == "Current context") %>% 
  
  ggplot(aes(x = model, y = perc, fill = `Context-sense association`)) +
  geom_bar(stat = "identity", position = position_dodge(.9), alpha = .5) +
  facet_grid(~ condition, scales = "free_x") +
  scale_y_continuous(limits = c(NA, 100), breaks = seq(0, 100, 25)) +
  geom_hline(yintercept=50, linetype="dashed", size = 0.7) + 
  labs(y = "Dominant sense selections (%)", x = " ", title = "Euclidean Distance Calculation Example\nRabagliati et al. (2013) - Experiment 1") +
  #coord_flip() +
  theme_bw() +
  scale_fill_grey() +
  #guides(fill = "none") +
  geom_segment(aes(x=1, y=95, xend=3, yend=95), size =1) +
  geom_segment(aes(x=1, y=90, xend=1, yend=95), size =1) +
  geom_segment(aes(x=3, y=90, xend=3, yend=95), size =1) +
  annotate("text", x=.75, y=88, label= "85", size = 6) + 
  annotate("text", x=1.25, y=18, label= "15", size = 6) + 
  annotate("text", x=1.75, y=73, label= "69", size = 6) + 
  annotate("text", x=2.25, y=42, label= "38", size = 6) + 
  annotate("text", x=2.75, y=84, label= "81", size = 6) +
    annotate("text", x=3.25, y=42, label= "38", size = 6) + 
  annotate("text", x=2, y=98, label= "sqrt( (85 - 81)^2 + (15 - 38)^2 ) = 23", size = 8) + 
  geom_segment(aes(x=2, y=65, xend=3, yend=65), size =1) +
  geom_segment(aes(x=2, y=61, xend=2, yend=65), size =1) +
  geom_segment(aes(x=3, y=61, xend=3, yend=65), size =1) +
  annotate("text", x=2.5, y=70, label= "sqrt( (69 - 81)^2 + (38 - 38)^2 ) = 12", size = 8) + 
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25))
```



\newline

```{r rabagliati_1_modelsize_plots, echo=FALSE, fig.width = 17, fig.height=8, dpi=300}
bert_output %>%
  mutate(value = case_when(
    correct == "bow_weapon" ~ 1,
    correct == "bow_knot" ~ 0,
    correct == "line_order" ~ 1,
    correct == "line_geometry" ~ 0,
    TRUE ~ value
  )) %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>%
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(correct = case_when(
    correct == "subordinate" ~ "Subordinate\nplausible",
    correct == "primary" ~ "Dominant\nplausible"
  )) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         correct = factor(correct),
         resp = factor(resp)) %>%
  group_by(model, condition, 
           correct, resp) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, correct, resp, fill = list(n= 0)) %>% 
  group_by(model, condition, 
           correct) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>%
  mutate(perc = case_when(
    condition == "current" & str_detect(correct, "Dominant") ~ (perc - current_sentence_primary_plausible)^2,
    condition == "current" & str_detect(correct, "Subordinate") ~ (perc - current_sentence_subordinate_plausible)^2,
    condition == "prior" & str_detect(correct, "Dominant") ~ (perc - prior_sentence_primary_plausible)^2,
    condition == "prior" & str_detect(correct, "Subordinate") ~ (perc - prior_sentence_subordinate_plausible)^2
  )) %>% 
  group_by(model, condition) %>% 
  summarise(primary_diff = sqrt(perc[which(str_detect(correct, "Dominant"))] + 
              perc[which(str_detect(correct, "Subordinate"))])) %>%
  ungroup %>%
  mutate(condition = str_to_title(condition) %>%
           str_replace("$", " context")) %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by model size
    new_levels <- df %>%
      arrange(size) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(size), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
  geom_point(aes(colour = Family), size = 8, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  geom_smooth(aes(colour = Family), method = "lm", se = FALSE) +
  
  facet_grid(~ condition, scales = "free_x") +
  labs(y = "Euclidean Distance (Model vs. Children)", 
       x = "Model size (log million parameters)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) + 
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25))
```

\newline

```{r rabagliati_1_pretrainingsize_plots, echo=FALSE, fig.width = 17, fig.height=8, dpi=300}
set.seed(1)
bert_output %>%
  mutate(value = case_when(
    correct == "bow_weapon" ~ 1,
    correct == "bow_knot" ~ 0,
    correct == "line_order" ~ 1,
    correct == "line_geometry" ~ 0,
    TRUE ~ value
  )) %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>%
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(correct = case_when(
    correct == "subordinate" ~ "Subordinate\nplausible",
    correct == "primary" ~ "Dominant\nplausible"
  )) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         correct = factor(correct),
         resp = factor(resp)) %>%
  group_by(model, condition, 
           correct, resp) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, correct, resp, fill = list(n= 0)) %>% 
  group_by(model, condition, 
           correct) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>%
  filter(resp != "subordinate") %>%
  mutate(perc = case_when(
    condition == "current" & str_detect(correct, "Dominant") ~ (perc - current_sentence_primary_plausible)^2,
    condition == "current" & str_detect(correct, "Subordinate") ~ (perc - current_sentence_subordinate_plausible)^2,
    condition == "prior" & str_detect(correct, "Dominant") ~ (perc - prior_sentence_primary_plausible)^2,
    condition == "prior" & str_detect(correct, "Subordinate") ~ (perc - prior_sentence_subordinate_plausible)^2
  )) %>% 
  group_by(model, condition) %>% 
  summarise(primary_diff = sqrt(perc[which(str_detect(correct, "Dominant"))] + 
              perc[which(str_detect(correct, "Subordinate"))])) %>%
  ungroup %>%
  mutate(condition = str_to_title(condition) %>%
           str_replace("$", " context")) %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by pretraining size
    new_levels <- df %>%
      arrange(pretraining) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(pretraining), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  geom_jitter(aes(colour = Family), size = 8, alpha = .6, height = 0.15) +
  facet_grid(~ condition, scales = "free_x") +
  labs(y = "Euclidean Distance (Model vs. Children)", 
       x = "Pretraining size (log GB)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) + 
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25))
```

\newline

```{r echo=FALSE}
mod_data <- 
  bert_output %>%
  mutate(value = case_when(
    correct == "bow_weapon" ~ 1,
    correct == "bow_knot" ~ 0,
    correct == "line_order" ~ 1,
    correct == "line_geometry" ~ 0,
    TRUE ~ value
  )) %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>%
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(correct = case_when(
    correct == "subordinate" ~ "Subordinate\nplausible",
    correct == "primary" ~ "Dominant\nplausible"
  )) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         correct = factor(correct),
         resp = factor(resp)) %>%
  group_by(model, condition, 
           correct, resp) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, correct, resp, fill = list(n= 0)) %>% 
  group_by(model, condition, 
           correct) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>%
  filter(resp != "subordinate") %>%
  mutate(perc = case_when(
    condition == "current" & str_detect(correct, "Dominant") ~ (perc - current_sentence_primary_plausible)^2,
    condition == "current" & str_detect(correct, "Subordinate") ~ (perc - current_sentence_subordinate_plausible)^2,
    condition == "prior" & str_detect(correct, "Dominant") ~ (perc - prior_sentence_primary_plausible)^2,
    condition == "prior" & str_detect(correct, "Subordinate") ~ (perc - prior_sentence_subordinate_plausible)^2
  )) %>% 
  group_by(model, condition) %>% 
  summarise(primary_diff = sqrt(perc[which(str_detect(correct, "Dominant"))] + 
              perc[which(str_detect(correct, "Subordinate"))])) %>%
  ungroup %>%
  mutate(condition = str_to_title(condition) %>%
           str_replace("$", " context")) %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  mutate(family = factor(family, levels = family_levels)) %>%
  left_join(primary_preference, by = "model")%>%
  rename(`model size` = size,
         `pretraining size` = pretraining,
         `dominant bias` = preference)
```

```{r echo=FALSE}
  # LINEAR MIXED-EFFECT MODELs
`Null model` <- `Main effects` <- lmer(primary_diff ~  1 + 
                         (1|family), data = mod_data)

`+ Dominant Bias` <- `Main effects` <- lmer(primary_diff ~  `dominant bias` + 
                         (1|family), data = mod_data)


`+ Condition` <- lmer(primary_diff ~  condition + 
                        `dominant bias` +
                         (1|family), data = mod_data)


`+ Pretraining` <- lmer(primary_diff ~
                         log(`pretraining size`) + 
                         condition + 
                          `dominant bias` +
                         (1|family), data = mod_data)


`+ Model size` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                         condition + 
                         `dominant bias` +
                         (1|family), data = mod_data)

`+ Pretraining*Condition` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                                    condition + 
                           `dominant bias` +
                                    (condition * log(`pretraining size`)) + 
                                    (1|family), data = mod_data)


`+ Size*Condition` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                             condition + 
                           `dominant bias` +
                             (condition * log(`pretraining size`)) + 
                             (condition *log(`model size`)) + 
                             (1|family), data = mod_data)



`+ Pretraining*Model size` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                             condition + 
                           `dominant bias` +
                             (condition * log(`pretraining size`)) + 
                             (condition *log(`model size`)) + 
                               (log(`model size`) * log(`pretraining size`)) +
                             (1|family), data = mod_data)


anova(`Null model`,
      `+ Dominant Bias`,
      `+ Condition`,
      `+ Pretraining`,
      `+ Model size`,
      `+ Pretraining*Condition`,
      `+ Size*Condition`,
      `+ Pretraining*Model size`
      ) %>% 
  round_anova %>%
  kable
```

\newline

```{r echo= FALSE}
tab_model(`+ Pretraining*Condition`, dv.labels = "Pretraining * Condition model<br>Rabagliati et al. (2013) - experiment 1")

## DIAGNOSTICS
# simulate residuals
#simulationOutput <- simulateResiduals(fittedModel = `+ Size*Condition`, plot = F)

# qqplot and residual plots
#plot(simulationOutput, quantreg = T)

# check uniformity, dispersion and outliers (no bootstrap)
#testResiduals(simulationOutput)

# check outliers
#testOutliers(simulationOutput,  type = "bootstrap")

# check inflation
#testZeroInflation(simulationOutput)
#countOnes <- function(x) sum(x == 1)  # testing for number of 1s
#testGeneric(simulationOutput, summary = countOnes, alternative = "greater") # 1-inflation

# plot residuals agaist individual predictors
#plotResiduals(simulationOutput, log(mod_data$pretraining), quantreg = T)

# residuals against random effect of id
#simulationOutput = recalculateResiduals(simulationOutput , group = model_df2$id)
#testDispersion(simulationOutput)

#car::vif()

```


```{r echo=FALSE}
# approximate maximum differences in Dominant sense selections in children (Rabagliati et al., 2013; Experiment 2)
# extracted from the paper's plots
sentence_primary_plausible <- 
  39
sentence_subordinate_plausible <- 
  21
```

\newline

```{r rabagliati_2_modelsize_pretrainingsize_plots, echo=FALSE, fig.width=17, fig.height=8, dpi=300}
gridExtra::grid.arrange(
# Rabagliati - competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "b") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>%  
  mutate(response = gold == sense) %>% 
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  arrange(model, experiment, utterance, condition) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         response = factor(response)) %>% 
  group_by(model, condition, response, .drop = FALSE) %>%
  summarise(n = length(response)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>%
  mutate(perc = round(n / tot * 100, 0)) %>% 
  mutate(sense = case_when(
    response == FALSE & condition == "associated" ~ "primary",
    response == TRUE & condition == "unassociated" ~ "primary",
    TRUE ~ "subordinate"
  )) %>% 
  filter(sense != "subordinate") %>%
  mutate(condition = case_when(
    condition == "associated" ~ "Subordinate\nplausible",
    condition == "unassociated" ~ "Dominant\nplausible"
  ) %>%
  factor(levels = c("Subordinate\nplausible", "Dominant\nplausible"))) %>%  
  mutate(perc = case_when(
      condition == "Subordinate\nplausible" ~ (perc - sentence_subordinate_plausible)^2,
      condition == "Dominant\nplausible" ~ (perc - sentence_primary_plausible)^2
    )) %>% 
  group_by(model) %>%
  summarise(primary_diff = sqrt(perc[which(str_detect(condition, "Dominant"))] +
              perc[which(str_detect(condition, "Subordinate"))])) %>%
  ungroup %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by model size
    new_levels <- df %>%
      arrange(size) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
   mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>%
  
  ggplot(aes(x = log(size), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
  geom_point(aes(colour = Family), size = 7, alpha = .5) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  geom_smooth(aes(colour = Family), method = "lm", se = FALSE) +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Model size (log million parameters)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  
  scale_y_continuous(limits = c(0, NA)) +
  
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

# Rabagliati - competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "b") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>%  
  mutate(response = gold == sense) %>% 
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  arrange(model, experiment, utterance, condition) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         response = factor(response)) %>% 
  group_by(model, condition, response, .drop = FALSE) %>%
  summarise(n = length(response)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>%
  mutate(perc = round(n / tot * 100, 0)) %>% 
  mutate(sense = case_when(
    response == FALSE & condition == "associated" ~ "primary",
    response == TRUE & condition == "unassociated" ~ "primary",
    TRUE ~ "subordinate"
  )) %>% 
  filter(sense != "subordinate") %>%
  mutate(condition = case_when(
    condition == "associated" ~ "Subordinate\nplausible",
    condition == "unassociated" ~ "Dominant\nplausible"
  ) %>%
  factor(levels = c("Subordinate\nplausible", "Dominant\nplausible"))) %>%  
  mutate(perc = case_when(
      condition == "Subordinate\nplausible" ~ (perc - sentence_subordinate_plausible)^2,
      condition == "Dominant\nplausible" ~ (perc - sentence_primary_plausible)^2
    )) %>% 
  group_by(model) %>%
  summarise(primary_diff = sqrt(perc[which(str_detect(condition, "Dominant"))] +
              perc[which(str_detect(condition, "Subordinate"))])) %>%
  ungroup %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by pretraining size
    new_levels <- df %>%
      arrange(pretraining) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
   mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 
  
  ggplot(aes(x = log(pretraining), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
  geom_point(aes(colour = Family), size = 7, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Pretraining size (log GB)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  
  scale_y_continuous(limits = c(0, NA)) +
  
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

ncol = 2)
```




\newline

```{r echo=FALSE}
mod_data <- bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>% 
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Rabagliati", experiment == "b") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>%  
  mutate(response = gold == sense) %>% 
  mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  arrange(model, experiment, utterance, condition) %>% 
  mutate(model = factor(model),
         condition = factor(condition),
         response = factor(response)) %>% 
  group_by(model, condition, response, .drop = FALSE) %>%
  summarise(n = length(response)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>%
  mutate(perc = round(n / tot * 100, 0)) %>% 
  mutate(sense = case_when(
    response == FALSE & condition == "associated" ~ "primary",
    response == TRUE & condition == "unassociated" ~ "primary",
    TRUE ~ "subordinate"
  )) %>% 
  filter(sense != "subordinate") %>%
  mutate(condition = case_when(
    condition == "associated" ~ "Subordinate\nplausible",
    condition == "unassociated" ~ "Dominant\nplausible"
  ) %>%
  factor(levels = c("Subordinate\nplausible", "Dominant\nplausible"))) %>%  
  mutate(perc = case_when(
      condition == "Subordinate\nplausible" ~ (perc - sentence_subordinate_plausible)^2,
      condition == "Dominant\nplausible" ~ (perc - sentence_primary_plausible)^2
    )) %>% 
  group_by(model) %>%
  summarise(primary_diff = sqrt(perc[which(str_detect(condition, "Dominant"))] +
              perc[which(str_detect(condition, "Subordinate"))])) %>%
  ungroup %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
   mutate(family = factor(family, levels = family_levels))  %>%
  left_join(primary_preference, by = "model")%>%
  rename(`model size` = size,
         `pretraining size` = pretraining,
         `dominant bias` = preference)
```

```{r echo= FALSE}
`Null model` <- lmer(primary_diff ~  1 + (1|family), data = mod_data)

`+ Dominant Bias` <- lmer(primary_diff ~  `dominant bias` + (1|family), data = mod_data)

`+ Pretraining` <- lmer(primary_diff ~  log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)

`+ Model size` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                         `dominant bias` +
                         (1|family), data = mod_data)

`+ Model size * Pretraining` <- lmer(primary_diff ~  log(`model size`) * 
                         log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)

anova(`Null model`, 
      `+ Dominant Bias`,
      `+ Pretraining`,
      `+ Model size`,
      `+ Model size * Pretraining`) %>%
  round_anova %>%
  kable

```

\newline

```{r echo = FALSE}
tab_model(`+ Model size`, dv.labels = "Main effects model<br>Rabagliati et al. (2013) - Experiment 2")

## DIAGNOSTICS
# simulate residuals
#simulationOutput <- simulateResiduals(fittedModel = `Main effects`, plot = F)

# qqplot and residual plots
#plot(simulationOutput, quantreg = T)

# check uniformity, dispersion and outliers (no bootstrap)
#testResiduals(simulationOutput)

# check outliers
#testOutliers(simulationOutput,  type = "bootstrap")

# check inflation
#testZeroInflation(simulationOutput)
#countOnes <- function(x) sum(x == 1)  # testing for number of 1s
#testGeneric(simulationOutput, summary = countOnes, alternative = "greater") # 1-inflation

# plot residuals agaist individual predictors
#plotResiduals(simulationOutput, log(mod_data$size), quantreg = T)

# residuals against random effect of id
#simulationOutput = recalculateResiduals(simulationOutput , group = model_df2$id)
#testDispersion(simulationOutput)

#car::vif(`Main effects`)
```

\newline

```{r echo=FALSE, fig.width =17, fig.height=20, dpi=300}
cabiddu_control <- 26
cabiddu_lexical <- 60
cabiddu_semantic <- 62
```

\newline

```{r echo=FALSE, fig.width =17, fig.height=8, dpi=300}
gridExtra::grid.arrange(
  # Cabiddu competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>% 
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
     # mean(
    #    c(
     #     perc[which(condition == "Lexical")]
    #      ,
           perc[which(condition == "Semantic")]
    #      )
     #   )
              )) %>% 
  ungroup %>%
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by model size
    new_levels <- df %>%
      arrange(size) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(size), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
    geom_point(aes(colour = Family), size = 5, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  geom_smooth(aes(colour = Family), method = "lm", se = FALSE) +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Model size (log million parameters)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

# Cabiddu competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
      #mean(
      #  c(
        #  perc[which(condition == "Lexical")]
       #   ,
           perc[which(condition == "Semantic")]
      #    )
       # )
              )) %>% 
  ungroup %>%
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by pretraining size
    new_levels <- df %>%
      arrange(pretraining) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(pretraining), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
    geom_point(aes(colour = Family), size = 5, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Pretraining size (log GB)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

ncol = 2
)
```


\newline

```{r echo=FALSE}
mod_data<- bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
      #mean(
      #  c(
       #   perc[which(condition == "Lexical")]
       #   ,
           perc[which(condition == "Semantic")]
      #    )
      #  )
              )) %>% 
  ungroup %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  left_join(primary_preference, by = "model")%>%
  rename(`model size` = size,
         `pretraining size` = pretraining,
         `dominant bias` = preference)
```

```{r echo = FALSE}
`Null model` <- lmer(primary_diff ~  1 + (1|family), data = mod_data)

`+ Dominant Bias` <- lmer(primary_diff ~  `dominant bias`  + (1|family), data = mod_data)

`+ Pretraining` <- lmer(primary_diff ~ log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)

`+ Model size` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                         `dominant bias` +
                         (1|family), data = mod_data)

`+ Model size * Pretraining` <- lmer(primary_diff ~  log(`model size`) * log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)


anova(`Null model`, 
      `+ Dominant Bias`,
      `+ Pretraining`,
      `+ Model size`,
      `+ Model size * Pretraining`) %>%
  round_anova %>%
  kable
```

\newline

```{r echo = FALSE}
tab_model(`+ Pretraining`, dv.labels = "+ Pretraining model<br>Verb-Event vs. Control<br>Cabiddu et al. (2022b)")

## DIAGNOSTICS
# simulate residuals
#simulationOutput <- simulateResiduals(fittedModel = mod2, plot = F)

# qqplot and residual plots
#plot(simulationOutput, quantreg = T)

# check uniformity, dispersion and outliers (no bootstrap)
#testResiduals(simulationOutput)

# check outliers
#testOutliers(simulationOutput,  type = "bootstrap")

# check inflation
#testZeroInflation(simulationOutput)
#countOnes <- function(x) sum(x == 1)  # testing for number of 1s
#testGeneric(simulationOutput, summary = countOnes, alternative = "greater") # 1-inflation

# plot residuals agaist individual predictors
#plotResiduals(simulationOutput, log(mod_data$size), quantreg = T)

# residuals against random effect of id
#simulationOutput = recalculateResiduals(simulationOutput , group = model_df2$id)
#testDispersion(simulationOutput)

#car::vif(mod1)
```

\newline

```{r echo=FALSE, fig.width =17, fig.height=8, dpi=300}
gridExtra::grid.arrange(
  # Cabiddu competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>% 
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
     # mean(
    #    c(
          perc[which(condition == "Lexical")]
    #      ,
        #   perc[which(condition == "Semantic")]
    #      )
     #   )
              )) %>% 
  ungroup %>%
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by model size
    new_levels <- df %>%
      arrange(size) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(size), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
    geom_point(aes(colour = Family), size = 5, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  geom_smooth(aes(colour = Family), method = "lm", se = FALSE) +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Model size (log million parameters)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

# Cabiddu competition
bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
      #mean(
      #  c(
          perc[which(condition == "Lexical")]
       #   ,
         #  perc[which(condition == "Semantic")]
      #    )
       # )
              )) %>% 
  ungroup %>%
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  (function(df) {
    # by pretraining size
    new_levels <- df %>%
      arrange(pretraining) %>% 
      pull(model) %>% 
      unique
    
    df %>% 
      mutate(model = factor(model, levels = new_levels))
  }) %>% 
  mutate(family = factor(family, levels = family_levels)) %>% 
  rename(Family = family) %>% 

  ggplot(aes(x = log(pretraining), y = primary_diff)) +
  geom_hline(yintercept=0, linetype="solid", size = 0.7) + 
    geom_point(aes(colour = Family), size = 5, alpha = .7) +
  geom_smooth(method = "lm", se = TRUE, colour = "black", size = 2, linetype = "dashed") +
  labs(y = "Euclidean Distance (Model vs. Children)", x = "Pretraining size (log GB)") +
  theme_bw() +
  scale_colour_manual(values = my_palette) +
  theme(plot.title = element_text(size = 30), 
        axis.text.x = element_text(size=25, angle = 0, vjust = 0.7),
        axis.text.y = element_text(size=25),
        strip.text = element_text(size=20),
        legend.text = element_text(size = 20),
        legend.title = element_text(size = 20),
        axis.title.y = element_text(size=20, margin = margin(t = 0, r = 15, b = 0, l = 0)),
        axis.title.x = element_text(size=20, margin = margin(t = 30, r = 0, b = 0, l = 0)),
        plot.caption = element_text(hjust = 0, size = 25)),

ncol = 2
)
```

\newline

```{r echo=FALSE}
mod_data<- bert_output %>%
  mutate(correct = case_when(
    value == "0" ~ "subordinate",
    value == "1" ~ "primary",
    TRUE ~ correct
  )) %>%
  mutate(cosine = as.numeric(cosine)) %>%
  filter(study == "Cabiddu", experiment == "a") %>% 
  group_by(model, utterance) %>%
  filter(cosine == max(cosine)) %>% 
  ungroup %>% 
  mutate(response = gold == sense) %>% mutate(resp = case_when(
    response == TRUE ~ correct,
    response == FALSE & correct == "subordinate" ~ "primary",
    response == FALSE & correct == "primary" ~ "subordinate"
  )) %>% 
  mutate(condition = str_to_title(condition)) %>%
  mutate(model = factor(model),
         condition = factor(condition),
         resp = factor(resp)) %>%
  group_by(model, condition, resp, .drop = FALSE) %>%
  summarise(n = length(resp)) %>%
  ungroup %>% 
  complete(model, condition, resp, fill = list(n= 0)) %>%
  group_by(model, condition) %>%
  mutate(tot = sum(n)) %>%
  ungroup %>% 
  mutate(perc = round(n / tot * 100, 0)) %>% 
  filter(resp != "subordinate") %>% 
    mutate(perc = case_when(
    condition == "Control" ~ (perc - cabiddu_control)^2,
    condition == "Lexical" ~ (perc - cabiddu_lexical)^2,
    condition == "Semantic" ~ (perc - cabiddu_semantic)^2,
  )) %>%  
  group_by(model) %>% 
  summarise(primary_diff = sqrt(
    perc[which(condition == "Control")] +
      #mean(
      #  c(
          perc[which(condition == "Lexical")]
       #   ,
         #  perc[which(condition == "Semantic")]
      #    )
      #  )
              )) %>% 
  ungroup %>% 
  left_join(family_codes, by = c("model" = "models")) %>% 
  mutate(model = factor(model, levels = full_model_levels)) %>%
  left_join(primary_preference, by = "model")%>%
  rename(`model size` = size,
         `pretraining size` = pretraining,
         `dominant bias` = preference)
```

```{r echo = FALSE}
`Null model` <- lmer(primary_diff ~  1 + (1|family), data = mod_data)

`+ Dominant Bias` <- lmer(primary_diff ~  `dominant bias`  + (1|family), data = mod_data)

`+ Pretraining` <- lmer(primary_diff ~ log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)

`+ Model size` <- lmer(primary_diff ~  log(`model size`) + 
                         log(`pretraining size`) + 
                         `dominant bias` +
                         (1|family), data = mod_data)

`+ Model size * Pretraining` <- lmer(primary_diff ~  log(`model size`) * log(`pretraining size`) + 
                          `dominant bias` +
                          (1|family), data = mod_data)


anova(`Null model`, 
      `+ Dominant Bias`,
      `+ Pretraining`,
      `+ Model size`,
      `+ Model size * Pretraining`) %>%
  round_anova %>%
  kable
```

\newline

```{r echo = FALSE}
tab_model(`+ Pretraining`, dv.labels = "+ Pretraining model<br>Verb-Lexical vs. Control<br>Cabiddu et al. (2022b)")

## DIAGNOSTICS
# simulate residuals
#simulationOutput <- simulateResiduals(fittedModel = mod2, plot = F)

# qqplot and residual plots
#plot(simulationOutput, quantreg = T)

# check uniformity, dispersion and outliers (no bootstrap)
#testResiduals(simulationOutput)

# check outliers
#testOutliers(simulationOutput,  type = "bootstrap")

# check inflation
#testZeroInflation(simulationOutput)
#countOnes <- function(x) sum(x == 1)  # testing for number of 1s
#testGeneric(simulationOutput, summary = countOnes, alternative = "greater") # 1-inflation

# plot residuals agaist individual predictors
#plotResiduals(simulationOutput, log(mod_data$size), quantreg = T)

# residuals against random effect of id
#simulationOutput = recalculateResiduals(simulationOutput , group = model_df2$id)
#testDispersion(simulationOutput)

#car::vif(mod1)
```

